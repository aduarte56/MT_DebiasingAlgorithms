{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessingEmbeddings import *\n",
    "from HardDebias import *\n",
    "from DoubleHardDebias import *\n",
    "from INLP import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.svm import LinearSVC, SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove-wiki-gigaword-50 embeddings\n",
      "vectors shape: (400000, 50), word2idx length: 400000, vocab length: 400000\n"
     ]
    }
   ],
   "source": [
    "glove=Embeddings('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=glove.vectors\n",
    "word2idx=glove.word2idx\n",
    "vocab=glove.words\n",
    "dict_vectors = glove.get_word_vector_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(vectors).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender specific vocabulary:\n",
    "gender_specific=[]\n",
    "with open('./Data/male_word_file.txt') as f:\n",
    "    gender_specific = [line.strip() for line in f]\n",
    "\n",
    "with open('./Data/female_word_file.txt') as f:\n",
    "    for l in f:\n",
    "        gender_specific.append(l.strip())\n",
    "\n",
    "with codecs.open('./Data/gender_specific_full.json') as f:\n",
    "    gender_specific.extend(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:00<00:00, 631139.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of limited vocabulary: 326614\n"
     ]
    }
   ],
   "source": [
    "# Getting a limited vocabulary to debias the embeddings.\n",
    "vocab_cleaned, vectors_cleaned, word2idx_cleaned, dict_vec_cleaned = glove.limit_vocab(vectors, word2idx, vocab, exclude=gender_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the definitional sets to calculate afterwards the gender direction. The first 10 gender sets were proposed by Bolukbasi et al. (2016)\n",
    "#Definitional sets for race where proposed by Manzini et al. in Multiclass debiasing of embeddings: https://github.com/TManzini/DebiasMulticlassWordEmbedding/blob/master/Debiasing/data/vocab/race_attributes_optm.json\n",
    "\n",
    "def_sets={\n",
    "    \"gender\" : [\n",
    "    ['she', 'he'], ['herself', 'himself'], ['her', 'his'], ['daughter', 'son'], ['girl', 'boy'],\n",
    "    ['mother', 'father'], ['woman', 'man'], ['mary', 'john'], ['gal', 'guy'], ['female', 'male'],['aunt', 'uncle']],\n",
    "    \n",
    "    \"race\":[\n",
    "\t\t[\"black\", \"caucasian\", \"asian\"],\n",
    "\t\t[\"african\", \"caucasian\", \"asian\"],\n",
    "\t\t[\"black\", \"white\", \"asian\"],\n",
    "\t\t[\"africa\", \"america\", \"asia\"],\n",
    "\t\t[\"africa\", \"america\", \"china\"],\n",
    "\t\t[\"africa\", \"europe\", \"asia\"]\n",
    "    ], \n",
    "    \"religion\":[\n",
    "\t\t[\"judaism\", \"christianity\", \"islam\"],\n",
    "\t\t[\"jew\", \"christian\", \"muslim\"],\n",
    "    [\"synagogue\", \"church\", \"mosque\"],\n",
    "    [\"torah\", \"bible\", \"quran\"],\n",
    "    [\"rabbi\", \"priest\", \"imam\"]\n",
    "\t]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalizing_lists = {\n",
    "    \"gender\": [\n",
    "        [\"monastery\", \"convent\"], [\"spokesman\", \"spokeswoman\"], [\"Catholic_priest\", \"nun\"], [\"Dad\", \"Mom\"], [\"Men\", \"Women\"],\n",
    "        [\"councilman\", \"councilwoman\"], [\"grandpa\", \"grandma\"], [\"grandsons\", \"granddaughters\"], [\"prostate_cancer\", \"ovarian_cancer\"],\n",
    "        [\"testosterone\", \"estrogen\"], [\"uncle\", \"aunt\"], [\"wives\", \"husbands\"], [\"Father\", \"Mother\"], [\"Grandpa\", \"Grandma\"],\n",
    "        [\"He\", \"She\"], [\"boy\", \"girl\"], [\"boys\", \"girls\"], [\"brother\", \"sister\"], [\"brothers\", \"sisters\"], [\"businessman\", \"businesswoman\"],\n",
    "        [\"chairman\", \"chairwoman\"], [\"colt\", \"filly\"], [\"congressman\", \"congresswoman\"], [\"dad\", \"mom\"], [\"dads\", \"moms\"], [\"dudes\", \"gals\"],\n",
    "        [\"ex_girlfriend\", \"ex_boyfriend\"], [\"father\", \"mother\"], [\"fatherhood\", \"motherhood\"], [\"fathers\", \"mothers\"], [\"fella\", \"granny\"],\n",
    "        [\"fraternity\", \"sorority\"], [\"gelding\", \"mare\"], [\"gentleman\", \"lady\"], [\"gentlemen\", \"ladies\"], [\"grandfather\", \"grandmother\"],\n",
    "        [\"grandson\", \"granddaughter\"], [\"he\", \"she\"], [\"himself\", \"herself\"], [\"his\", \"her\"], [\"king\", \"queen\"], [\"kings\", \"queens\"],\n",
    "        [\"male\", \"female\"], [\"males\", \"females\"], [\"man\", \"woman\"], [\"men\", \"women\"], [\"nephew\", \"niece\"], [\"prince\", \"princess\"], \n",
    "        [\"schoolboy\", \"schoolgirl\"], [\"son\", \"daughter\"], [\"sons\", \"daughters\"], [\"twin_brother\", \"twin_sister\"]],\n",
    "\n",
    "    \"race\": [\n",
    "        [\"black\", \"caucasian\", \"asian\"],\n",
    "      \t\t[\"african\", \"caucasian\", \"asian\"],\n",
    "      \t\t[\"black\", \"white\", \"asian\"],\n",
    "      \t\t[\"africa\", \"america\", \"asia\"],\n",
    "      \t\t[\"africa\", \"america\", \"china\"],\n",
    "      \t\t[\"africa\", \"europe\", \"asia\"]\n",
    "    ],\n",
    "    \"def_sets_religion\": [\n",
    "        [\"judaism\", \"christianity\", \"islam\"],\n",
    "      \t\t[\"jew\", \"christian\", \"muslim\"],\n",
    "        [\"synagogue\", \"church\", \"mosque\"],\n",
    "        [\"torah\", \"bible\", \"quran\"],\n",
    "        [\"rabbi\", \"priest\", \"imam\"]\n",
    "    ]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equalizing pairs were first published by Bolukbasi et al. in https://github.com/tolga-b/debiaswe/blob/master/data/equalize_pairs.json\n",
    "equalizing_list=[[\"monastery\", \"convent\"], [\"spokesman\", \"spokeswoman\"], [\"Catholic_priest\", \"nun\"], [\"Dad\", \"Mom\"], [\"Men\", \"Women\"], [\"councilman\", \"councilwoman\"], [\"grandpa\", \"grandma\"], [\"grandsons\", \"granddaughters\"], [\"prostate_cancer\", \"ovarian_cancer\"], [\"testosterone\", \"estrogen\"], [\"uncle\", \"aunt\"], [\"wives\", \"husbands\"], [\"Father\", \"Mother\"], [\"Grandpa\", \"Grandma\"], [\"He\", \"She\"], [\"boy\", \"girl\"], [\"boys\", \"girls\"], [\"brother\", \"sister\"], [\"brothers\", \"sisters\"], [\"businessman\", \"businesswoman\"], [\"chairman\", \"chairwoman\"], [\"colt\", \"filly\"], [\"congressman\", \"congresswoman\"], [\"dad\", \"mom\"], [\"dads\", \"moms\"], [\"dudes\", \"gals\"], [\"ex_girlfriend\", \"ex_boyfriend\"], [\"father\", \"mother\"], [\"fatherhood\", \"motherhood\"], [\"fathers\", \"mothers\"], [\"fella\", \"granny\"], [\"fraternity\", \"sorority\"], [\"gelding\", \"mare\"], [\"gentleman\", \"lady\"], [\"gentlemen\", \"ladies\"], [\"grandfather\", \"grandmother\"], [\"grandson\", \"granddaughter\"], [\"he\", \"she\"], [\"himself\", \"herself\"], [\"his\", \"her\"], [\"king\", \"queen\"], [\"kings\", \"queens\"], [\"male\", \"female\"], [\"males\", \"females\"], [\"man\", \"woman\"], [\"men\", \"women\"], [\"nephew\", \"niece\"], [\"prince\", \"princess\"], [\"schoolboy\", \"schoolgirl\"], [\"son\", \"daughter\"], [\"sons\", \"daughters\"], [\"twin_brother\", \"twin_sister\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-Debias Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the gender bias, we need to get the embeddings of \"he\" and \"she\"\n",
    "he_embed = dict_vectors['he']\n",
    "she_embed = dict_vectors['she']\n",
    "\n",
    "# Using the gender bias function to compute the bias of all the words in the limited dataset\n",
    "#We create a dictionary with the word as key and the bias as value\n",
    "gender_bias_original = utils.compute_bias(dict_vec_cleaned, he_embed, she_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_def_sets_subspace(list_def_sets):\n",
    "  def_sets={i: v for i, v in enumerate(list_def_sets)}\n",
    "  return def_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_set_gender=utils.prepare_def_sets_subspace(def_sets[\"gender\"])\n",
    "def_set_race=utils.prepare_def_sets_subspace(def_sets[\"race\"])\n",
    "def_set_religion=utils.prepare_def_sets_subspace(def_sets[\"religion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HardDebias as hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words considered: 2\n",
      "Running PCA with 1 components\n"
     ]
    }
   ],
   "source": [
    "debiased_vectors, debiased_vocab, debiased_word2idx= hard_debias(vectors,\n",
    "                             dict_vec_cleaned, \n",
    "                             word2idx_cleaned,\n",
    "                             vocab_cleaned, \n",
    "                             equalizing_list, \n",
    "                             def_set_gender,\n",
    "                             1\n",
    "                             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Double-Hard Debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_w2i, biased_vocab, female_words, male_words, y_true=utils.getting_biased_words(gender_bias_original, def_sets[\"def_sets_gender\"], 1000,word2idx_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(vectors).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component: 0, "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m precisions, optimal_direction \u001b[39m=\u001b[39m getting_optimal_direction(vectors, word2idx, biased_w2i, biased_vocab, male_words, female_words, y_true,def_sets[\u001b[39m\"\u001b[39;49m\u001b[39mdef_sets_gender\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Python/VersionControl/MT_DebiasingAlgorithms/DoubleHardDebias.py:60\u001b[0m, in \u001b[0;36mgetting_optimal_direction\u001b[0;34m(vectors, word2idx, w2i_partial, vocab_partial, male_words, female_words, y_true, definitional_pairs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m component_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m     58\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mComponent: \u001b[39m\u001b[39m{\u001b[39;00mcomponent_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m   wv_debiased \u001b[39m=\u001b[39m double_hard_debias(vectors, word2idx, w2i_partial, vocab_partial, component_ids \u001b[39m=\u001b[39;49m [component_id], definitional_pairs\u001b[39m=\u001b[39;49m definitional_pairs)\n\u001b[1;32m     61\u001b[0m   _, _, _, precision \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcluster_and_evaluate(male_words \u001b[39m+\u001b[39m female_words, \n\u001b[1;32m     62\u001b[0m                         utils\u001b[39m.\u001b[39mextract_vectors(male_words \u001b[39m+\u001b[39m female_words, wv_debiased, w2i_partial), \u001b[39m1\u001b[39m, y_true)\n\u001b[1;32m     63\u001b[0m   precisions\u001b[39m.\u001b[39mappend(precision)\n",
      "File \u001b[0;32m~/Python/VersionControl/MT_DebiasingAlgorithms/DoubleHardDebias.py:48\u001b[0m, in \u001b[0;36mdouble_hard_debias\u001b[0;34m(wv, w2i, w2i_partial, vocab_partial, component_ids, definitional_pairs)\u001b[0m\n\u001b[1;32m     45\u001b[0m vectors\u001b[39m=\u001b[39mremove_frequency_features(vocab_partial, wv, w2i, w2i_partial, component_ids)\n\u001b[1;32m     47\u001b[0m \u001b[39m# debias\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m gender_directions\u001b[39m=\u001b[39mfind_gender_direction(vectors, w2i_partial, definitional_pairs)\n\u001b[1;32m     50\u001b[0m wv_debiased\u001b[39m=\u001b[39mremove_gender_component(vocab_partial, vectors, w2i_partial, gender_directions) \n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m wv_debiased\n",
      "File \u001b[0;32m~/Python/VersionControl/MT_DebiasingAlgorithms/DoubleHardDebias.py:8\u001b[0m, in \u001b[0;36mfind_gender_direction\u001b[0;34m(word_vectors, word2index_partial, definitional_pairs)\u001b[0m\n\u001b[1;32m      6\u001b[0m gender_directions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m gender_word_list \u001b[39min\u001b[39;00m [definitional_pairs]:\n\u001b[0;32m----> 8\u001b[0m     gender_directions\u001b[39m.\u001b[39mappend(utils\u001b[39m.\u001b[39;49mperform_PCA_pairs(gender_word_list, word_vectors, word2index_partial)\u001b[39m.\u001b[39mcomponents_[\u001b[39m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m  gender_directions\n",
      "File \u001b[0;32m~/Python/VersionControl/MT_DebiasingAlgorithms/utils.py:165\u001b[0m, in \u001b[0;36mperform_PCA_pairs\u001b[0;34m(pairs, word_vectors, word2index, num_components)\u001b[0m\n\u001b[1;32m    163\u001b[0m matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(vector_matrix)\n\u001b[1;32m    164\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components \u001b[39m=\u001b[39m num_components)\n\u001b[0;32m--> 165\u001b[0m pca\u001b[39m.\u001b[39;49mfit(matrix)\n\u001b[1;32m    166\u001b[0m \u001b[39m#print('pairs used in PCA: ', count)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m#print(pca.explained_variance_ratio_)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m pca\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MT_env/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:435\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 435\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    436\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MT_env/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    486\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    489\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MT_env/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/MT_env/lib/python3.10/site-packages/sklearn/utils/validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    907\u001b[0m         )\n\u001b[1;32m    909\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "precisions, optimal_direction = getting_optimal_direction(vectors, word2idx, biased_w2i, biased_vocab, male_words, female_words, y_true,def_sets[\"def_sets_gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimal_direction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dh_debiased_vectors\u001b[39m=\u001b[39m double_hard_debias(vectors, word2idx, word2idx, vocab, [optimal_direction], def_sets[\u001b[39m\"\u001b[39m\u001b[39mdef_sets_gender\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimal_direction' is not defined"
     ]
    }
   ],
   "source": [
    "dh_debiased_vectors= double_hard_debias(vectors, word2idx, word2idx, vocab, [optimal_direction], def_sets[\"def_sets_gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dh_debiased_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dh_debiased_vocab_limited, dh_debiased_vectors_limited, dh_debiased_word2idx_limited \u001b[39m=\u001b[39m glove\u001b[39m.\u001b[39mlimit_vocab(dh_debiased_vectors, word2idx, vocab, exclude \u001b[39m=\u001b[39m gender_specific)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dh_debiased_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "dh_debiased_vocab_limited, dh_debiased_vectors_limited, dh_debiased_word2idx_limited = glove.limit_vocab(dh_debiased_vectors, word2idx, vocab, exclude = gender_specific)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words considered: 2\n",
      "Running PCA with 1 components\n",
      "TOP MASC: ('vulva', 'ilirska', 'sopra', 'seacole', 'konstantinovna', 'phagan', 'ivanovna', 'chromatids', 'sistership', 'lactating', 'aldermanbury', 'chromatid', 'antipolis', 'masumeh', 'ruwart', 'benedicta', 'agia', 'dasi', 'triada', 'maharani', 'gothel', 'woolnoth', 'miscarries', 'popova', 'mizzenmast', 'catharina', 'czestochowa', 'nikolayevna', 'lehzen', 'desnuda', 'herøy', 'murten', 'caesonia', 'mulleavy', 'aghia', 'ludwika', 'ignacia', 'parvomay', 'ferroviario', 'gaitskill', 'tanztheater', 'skłodowska-curie', 'vissi', 'jumonji', 'philomela', 'mahatmya', 'grebenkina', 'rafaela', 'viletta', 'romanova')\n",
      "-------------------------\n",
      "TOP FEM: ('inienger', 'boonyaratkalin', 'abdulsalami', 'jets', 'sonthi', 'scuds', 'voreqe', 'technicals', 'strategists', 'fired', 'valiquette', 'ac-130', 'tactic', 'patriot', 'defensive', 'singirok', 'kassam', 'greifeld', 'offensive', 'boonyaratglin', 'staubach', 'tactics', 'dannatt', 'sacking', 'kfir', 'firing', 'linemen', 'abdulsalam', 'lillee', 'midlevel', 'strike', 'volcanologists', 'gen.', 'commentators', 'chiefs', 'postolos', 'interception', 'preemptive', 'receivers', 'emptive', 'pundits', 'counterattacks', 'gunships', 'feehery', 'pac-3', 'defense', 'incoming', 'quarterbacks', 'halutz', '35-billion')\n",
      "-------------------------\n",
      "Random Neutral: ('fabergé', 'hissatsu', 'centerback', 'abey', 'hise', 'manuvakola', 'homewood', 'moustafa', 'yeomen', 'kanat', '69.63', 'reinvested', 'russophile', 'strategoi', 'tagi', 'undissolved', 'pre-installed', 'argentum', 'melquiades', 'ketterer', 'fechner', 'matveyev', 'orrefors', 'poer', 'bogard', 'castroviejo', '45-29', 'gabbed', '15-april', 'livolsi', 'summiteers', 'koodo', 'vardhana', 'betacam', '500-page', 'noodle', 'casaroli', 'kangs', 'domenick', 'generalities', 'volkan', '103.28', 'popover', 'ratomir', 'freema', 'ostap', 'bilingue', 'tisma', '79.91', 'gorgon')\n"
     ]
    }
   ],
   "source": [
    "num_vectors_per_class = 7500\n",
    "\n",
    "#gender_direction=algorithm1.find_gender_direction(vectors, word2idx_cleaned)\n",
    "\n",
    "#gender_vecs = [glove.model[p[0]] - glove.model[p[1]] for p in gendered_pairs]\n",
    "#pca = PCA(n_components=1)\n",
    "#pca.fit(gender_vecs)\n",
    "gender_direction = identify_bias_subspace(dict_vec_cleaned, def_set_gender, 1, centralizing=True)\n",
    "gender_direction = np.squeeze(gender_direction)\n",
    "\n",
    "gender_unit_vec = gender_direction/np.linalg.norm(gender_direction)\n",
    "masc_words_and_scores, fem_words_and_scores, neut_words_and_scores = getting_classes_for_INLP(gender_vector=gender_direction, model=glove.model, n = 7500)\n",
    "\n",
    "masc_words, masc_scores = list(zip(*masc_words_and_scores))\n",
    "neut_words, neut_scores = list(zip(*neut_words_and_scores))\n",
    "fem_words, fem_scores = list(zip(*fem_words_and_scores))\n",
    "masc_vecs, fem_vecs = glove.get_vectors_from_list(masc_words), glove.get_vectors_from_list(fem_words)\n",
    "neut_vecs = glove.get_vectors_from_list(neut_words)\n",
    "\n",
    "n = min(3000, num_vectors_per_class)\n",
    "all_significantly_biased_words = masc_words[:n] + fem_words[:n]\n",
    "all_significantly_biased_vecs =  np.concatenate((masc_vecs[:n], fem_vecs[:n]))\n",
    "all_significantly_biased_labels = np.concatenate((np.ones(n, dtype = int),\n",
    "                                                  np.zeros(n, dtype = int)))\n",
    "\n",
    "all_significantly_biased_words, all_significantly_biased_vecs, all_significantly_biased_labels = sklearn.utils.shuffle(\n",
    "all_significantly_biased_words, all_significantly_biased_vecs, all_significantly_biased_labels)\n",
    "#print(np.random.choice(masc_words, size = 75))\n",
    "print(\"TOP MASC:\",masc_words[:50] )\n",
    "print(\"-------------------------\")\n",
    "print(\"TOP FEM:\", fem_words[:50])\n",
    "print(\"-------------------------\")\n",
    "print(\"Random Neutral:\", neut_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06651511,  0.15298505, -0.22629546,  0.01367087,  0.10383943,\n",
       "        0.19781574,  0.14934979,  0.02839268,  0.19078207, -0.06777252,\n",
       "        0.10532494, -0.11232515,  0.06692035, -0.10428741,  0.06449812,\n",
       "        0.03163765, -0.20662562, -0.00147265,  0.33190876,  0.14005051,\n",
       "       -0.00183823,  0.14094647,  0.00308689,  0.16779467,  0.10143815,\n",
       "        0.18567674, -0.02985709,  0.09640412,  0.06599609, -0.28146555,\n",
       "       -0.09386818,  0.17567203,  0.11368872,  0.05253632, -0.17035815,\n",
       "        0.0029247 ,  0.00165801,  0.06263107,  0.1276077 , -0.07693021,\n",
       "       -0.0652855 , -0.2570974 ,  0.20480825, -0.14751672, -0.0626659 ,\n",
       "        0.00278824,  0.0045029 , -0.34939256,  0.14731076, -0.10574462])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_direction = np.squeeze(gender_direction)\n",
    "bias_direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words considered: 2\n",
      "Running PCA with 1 components\n"
     ]
    }
   ],
   "source": [
    "gender_direction = identify_bias_subspace(\n",
    "    dict_vec_cleaned, def_set_gender, 1, centralizing=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_direction.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10788; Dev size: 4624; Test size: 6606\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.concatenate((masc_vecs, fem_vecs, neut_vecs), axis = 0)\n",
    "#X = (X - np.mean(X, axis = 0, keepdims = True)) / np.std(X, axis = 0)\n",
    "y_masc = np.ones(masc_vecs.shape[0], dtype = int)\n",
    "y_fem = np.zeros(fem_vecs.shape[0], dtype = int)\n",
    "y_neut = -np.ones(neut_vecs.shape[0], dtype = int)\n",
    "#y = np.concatenate((masc_scores, fem_scores, neut_scores))#np.concatenate((y_masc, y_fem))\n",
    "y = np.concatenate((y_masc, y_fem, y_neut))\n",
    "X_train_dev, X_test, y_train_dev, Y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "X_train, X_dev, Y_train, Y_dev = sklearn.model_selection.train_test_split(X_train_dev, y_train_dev, test_size = 0.3, random_state = 0)\n",
    "print(\"Train size: {}; Dev size: {}; Test size: {}\".format(X_train.shape[0], X_dev.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_clf = LinearSVC\n",
    "#gender_clf = SGDClassifier\n",
    "#gender_clf = LogisticRegression\n",
    "#gender_clf = LinearDiscriminantAnalysis\n",
    "#gender_clf = Perceptron\n",
    "\n",
    "params_svc = {'penalty': 'l2','fit_intercept': False, 'class_weight': None, \"dual\": False, 'random_state': 0}\n",
    "params_sgd = {'penalty': 'l2','fit_intercept': False, 'class_weight': None, 'max_iter': 1000, 'random_state': 0}\n",
    "params = params_svc\n",
    "#params = {'loss': 'hinge', 'n_jobs': 16, 'penalty': 'l2', 'max_iter': 2500, 'random_state': 0}\n",
    "#params = {}\n",
    "n = 35\n",
    "min_acc = 0\n",
    "dropout_rate = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration: 34, accuracy: 0.972318339100346: 100%|██████████| 35/35 [00:03<00:00,  9.57it/s]\n"
     ]
    }
   ],
   "source": [
    "P, rowspace_projs, Ws = get_debiasing_projection(gender_clf, params, n, 50, min_acc,\n",
    "                                    X_train, Y_train, X_dev, Y_dev, Y_train_main=None, Y_dev_main=None, \n",
    "                                       dropout_rate = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
